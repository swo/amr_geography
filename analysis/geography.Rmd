---
title: "Theory of use-resistance relationships over geographic scales"
author: "Scott Olesen"
output:
  pdf_document: default
header-includes:
    - \usepackage{bm}
    - \usepackage{mathrsfs}
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(
  fig.width=7, fig.height=4, fig.path='fig/',
  dev=c('pdf'),
  echo=FALSE, warning=FALSE, message=FALSE,
  cache=TRUE, autodep=TRUE)
pdf.options(useDingbats=FALSE, useKerning=FALSE)
```

# To add

- When does population, rather than use, control resistance?
    - Hospital vs. community for some organisms
    - Which school/day care or family?
- Prediction is that, when there is just nearly as much transmission within population as across, then you'll get a breakdown in the use-resistance relationship. In other words, in these cases, the thing that controls resistance is population, rather than use. That's because population-level resistance is determined by population-level use and only weakly modulated by sub-population use.
- Maybe "tranmission population" means that transmission is greater within the group than across groups. "Analytical population" is how we do the analysis. When we pick multiple analytical populations within one transmission population, then we get weak within-tranmission-population relationships.
- Find a way to talk about the geographic construction: if you have a ring, then mixing doesn't hurt as much as if you are a well-connected graph

## Outline

- Theory
  - Interactions between populations dilute $r ~ u$. Too small a scale gives low slopes.
  - Slopes, correlations, and $\sigma_x^2$. We expect slopes to be similar, so $r^2 : \sigma_x \to [-1, 1]$.
- MAUP
  - ECDC, IMS/NHSN, MarketScan/RO
  - Is there some level of aggregation that tends to deliver stronger slopes?
- Smaller scale
  - Medicare/RO: going down to HRRs isn't bad. I'm not sure this is very convincing.
- Figures
  - Model explanation, islands & ring illustration
  - Tuning parameters and connectivity; how connectivity affects the slope
  - Zonal & random samples, maybe in 3 separate plots? Or in one huge plot with lots of insets

# Introduction

## Background

- At what length scale should analyses be conducted, or should interventions be implemented?
- Experiments and cross-sectional studies suggest that the scale could be really small
    - Lit re: azithro experiments suggest that scales can be really small
    - UK UTI papers suggest that you can go from CCG to practice level and not see any real change
    - Our work suggested that relationships are the same at the state and HRR levels
    - Transmission is known to be strong at the level of families and schools/day cares
- In other cases, the population or setting, rather than individual use, appears to control resistance
    - E.g., our race thing, where non-whites get less drug and have more resistance
    - Hospitals, where the difference between hospital or not is much greater than individual use
    - Suggestions that doing an analysis might not be expected to work at all because the US is one big well-mixed pot
- But questions remain, and there a rigorous answer to this question should be attempted

## Approach

- We distinguish between an "analytical population", the individuals whose pooled use and resistance are the units of analysis, and "transmission populations". Transmission populations have proportionally more contacts within their group than outside their group: $\beta_{i \leftarrow i} / N_i > \beta_{i \leftarrow j} / N_j$ for all $j \neq i$.
- We assume there are some true transmission populations that we don't know *a priori*. The questions are:
    - What are the risks of making our analytical populations much smaller than the transmission populations?
    - Much larger?
    - Can we discover what the TPs are?
- We use theoretical models and empirical analyses to assess these questions
    - We use a dynamical model that exhibits S/R coexistence, modifying its geographical scales and units of analysis to determine the theoretical boundaries for analyses
        - We assume "equilibrated resistance", meaning that antibiotic use is slowly changing in time, that each population has access to the same pool of pathogen strains, that strains are equally fit in different populations (excepting antibiotic use), and that resistance levels have equilibrated
    - We then use empirical use/resistance data from the US and Europe to check if we are near this scale

# Methods

## Summary of approaches

We use *theoretical* and *empirical* approaches.

First, we run simulations where the length scale of transmission is known, and the analysis is varied between units shorter and longer than that scale.

Second, we re-analyze empirical, ecological-level data about use and resistance and determine which length scales of transmission are compatible with those data.

## Intuition behind the simulation approach

Consider a set of islands. The inhabitants only have contact with other members of their island. The inhabitants of each island have some levels of average antibiotic use and antibiotic resistance. For simplicity, imagine that the use-resistance pairs $(\tau, \rho)$ fall on a line, subject to some noise.

Clearly the correct scale of analysis is the island. Using a larger scale of analysis ---say, aggregating nearby islands and using their cross-island average use and resistance--- will decrease statistical power, as there are fewer independent units. On the other hand, too small a scale ---say, different populations on the same island--- means that the units are not independent, and the use-resistance relationship will be "washed out" by the population-average value.

To see this, consider (*i*) a "lower-using-than-average" islander with use $\tau_\mathrm{lo}$ on an island with average use $\tau_\mathrm{hi} > \tau_\mathrm{lo}$ and (*ii*) a "low-and-average" islander with use $\tau_\mathrm{lo}$ on an island with average use $\tau_\mathrm{lo}$. The lower-than-average islander, being surrounded by individuals with a higher average probability of resistant carriage, is more likely to carry a resistant strain than the low-and-average islander. Conversely, a "higher-using-than-average" islander will benefit from the lower resistance probability of their neighbors.

This logic helps explain why a hospitalized person, whether they are receiving antibiotics or not, is more likely to carry a resistant organism than a person with the same antibiotic use who is outside the hospital.

We will use simulations to quantify the change in the measured use-resistance relationship when changing the length scale of the analysis.

## Theory

### Within-host neutral (WHN) model

#### Base model

I will use a dynamical modeling framework, the within-host neutrality (WHN) model presented by [Davies *et al.*](https://www.biorxiv.org/content/early/2018/01/03/217232) (doi:10.1101/217232), thats exhibit coexistence between susceptible and resistant organisms, with some relationship between antibiotic use $\tau$ and resistance prevalence $\rho = R / (1 - X)$.

In this model, individuals may be uncolonized $X$, carry a susceptible $S$ or resistant $R$ organism, or be dual-carriers $D$. Carriers naturally clear at a rate $u$. Antibiotics are used at a rate $\tau$ that clears $S \to X$ and removes dual carriage $D \to R$. The fitness cost of resistance is encoded by $c$, which modulates the resistant strain's transmission.
$$
\begin{aligned}
\dot{S} &= \beta \frac{S + D}{N} X - (u + \tau) S - \beta \underbrace{(1-c)} \frac{R}{N} S \\
\dot{R} &= \beta (1-c) \frac{R}{N} X - u R + \tau D \\
\dot{D} &= \beta (1-c) \frac{R}{N} S - (u + \tau) D \\
\dot{X} &= -(\dot{S} + \dot{R} + \dot{D}) \\
N &= X + S + R + D
\end{aligned}
$$
For simplicity, I set $k = 1$ in these equations and in all the simulations. The braced term was missing in the bioRxiv manuscript.

#### Parameters

```{r whn_parameters}
whn_base_parms = list(
  beta = 4.0,
  u = 1.0,
  tau50 = 1.5 / 12
)

# swo: always using davies base parms, so just bake those into the cost function?
# tau50 is the tau at which resistance prevalence is 50%
cost_function = function(beta, u, tau50) {
  k = 1.0
  
  1/2*(3*beta*k*tau50 - (3*k - 2)*tau50^2 - (k - 2)*u^2 + (beta*k - 4*(k - 1)*tau50)*u - 
    sqrt(beta^2*k^2*tau50^2 + (k^2 - 4*k + 4)*tau50^4 + (k^2 - 4*k + 4)*u^4 - 2*(beta*k^2 -
    2*beta*k)*tau50^3 - 2*(beta*k^2 - 2*beta*k + 8*(k - 1)*tau50)*u^3 + (beta^2*k^2 -
    2*(k^2 + 12*k - 12)*tau50^2 + 2*(beta*k^2 + 6*beta*k)*tau50)*u^2 - 2*(beta^2*k^2*tau50 +
    8*(k - 1)*tau50^3 - (beta*k^2 + 6*beta*k)*tau50^2)*u))/(beta*k*tau50 - k*tau50^2 - k*u^2 +
    (beta*k - 2*k*tau50)*u)
}

whn_base_parms$cost = with(
  whn_base_parms,
  cost_function(beta, u, tau50)
)

whn_base_parms
```

#### Population model

There are $n$ populations, indexed $i \in [0; n - 1]$, each of size $N_i$. Changes in population membership are slow compared to the transmission dynamics, so each $N_i$ remains fixed.

In a first simulation, the populations are all transmission populations. A parameter $\epsilon \in [0, 1]$ controls the relative amount of interactions within versus between the transmission populations. Let $\beta'_{i \leftarrow j}$ be the the contacts for individuals in $i$ that are in $j$ so that $\beta_{i \leftarrow j} = \beta \beta'_{i \leftarrow j}$, where $\beta$ is a constant. We require that $\sum_j \beta'_{i \leftarrow j} = 1$. When tuning $\epsilon$:
$$
\begin{aligned}
\beta'_{i \leftarrow i} &= 1 - \epsilon (1 - 1 / n) \\
\beta'_{i \leftarrow j} &= \epsilon / n \qquad (i \neq j)
\end{aligned}
$$

The model is now
$$
\begin{aligned}
\dot{S}_i &= \sum_j \beta_{i \leftarrow j} \frac{S_j + D_j}{N_j} X_i - (u + \tau_i) S_i - (1-c) \sum_j \beta_{i \leftarrow j} \frac{R_j}{N_j} S_i \\
\dot{R}_i &= (1-c) \sum_j \beta_{i \leftarrow j} \frac{R_j}{N_j} X_i - u R_i + \tau_i D_i \\
\dot{D}_i &= (1-c) \sum_j \beta_{i \leftarrow j} \frac{R_j}{N_j} S_i - (u + \tau_i) D_i \\
\dot{X}_i &= -(\dot{S}_i + \dot{R}_i + \dot{D}_i) \\
N_i &= X_i + S_i + R_i + D_i
\end{aligned}
$$

#### Nested model

To simulated the effect of going to too small populations, we consider the situation where the analytical populations are nested within transmission populations.

To make the nested analytical populations behave like a single transmission population, we have three rules:

- Nested APs interact with one another according to their relative sizes.
- Other TPs interact with APs according to their relative sizes.
- Each AP interacts with other TPs the same way.

In equations, this is:
$$
\beta_{i_\mu \leftarrow j_\nu} = \frac{N_{j_\nu}}{N_j} \beta_{i \leftarrow j}
$$
If one of the two populations has no subpopulation, then the fraction $N / N$ is just unity.

This equations ensure that, as far as the transmission and clearance terms are concerned, the APs $i_\mu$ act like a single TP $i$. The one difference is that the treatment terms cannot be written with a "combined" use $\tau_i$ (except in the trivial case where $\tau_{i_\mu} = \tau_{i_\nu}$). So different compositions of S/R/D might result from separating the groups in this way.

```{r}
#swo: these are actually useful functions
ring_dist = function(i, j, n) pmin(abs(i - j), n - abs(i - j))

beta_ij = function(i, j, n, epsilon) {
  num = epsilon ** ring_dist(i, j, n)
  den = sum(epsilon ** ring_dist(0, 0:(n - 1), n))
  num / den
}
```

```{r patch_viz}
circle = function(cx, cy, r, n = 100) {
  theta = seq(0, 2 * pi, length.out = n)
  
  data_frame(
    order = 1:n,
    x = cx + r * cos(theta),
    y = cy + r * sin(theta)
  )
}

wedges_f = function(cx, cy, r, n_wedge, n_arc = 100) {
  if (n_wedge == 1) {
    theta = seq(0, 2 * pi, length.out = n_arc)
    return(data_frame(
      wedge = 1,
      x = cx + r * cos(theta),
      y = cy + r * sin(theta)
    ))
  }
  
  thetas = function(i) {
    2 * pi * seq((i - 1.5) / n_wedge, (i - 0.5) / n_wedge, length.out = n_arc)
  }
  
  data_frame(wedge = 1:n_wedge) %>%
    mutate(data = map(wedge, ~ data_frame(
      x = cx + r * c(0, cos(thetas(.))),
      y = cy + r * c(0, sin(thetas(.)))
    ))) %>%
    unnest()
}

ring_scale = Vectorize(function(i, n, scale = 1.0) {
  if (n == 1) return(0)
  
  # rescale so that i=1 -> -scale, i=max dist -> +scale
  dists = ring_dist(1, 1:n, n)
  approx(x = c(min(dists), max(dists)),
         y = scale * c(-1, 1),
         xout = ring_dist(i, 1, n))$y
})

ring_viz = function(n_wedge, r, R = 1.0) {
  n_circle = length(n_wedge)
  
  centers = data_frame(
    center = 1:n_circle,
    n_wedge = n_wedge,
    center_theta = seq(0, 2 * pi, length.out = n_circle + 1) %>% head(-1),
    center_x = R * cos(center_theta),
    center_y = R * sin(center_theta)
  )
  
  circles = centers %>%
    mutate(circles = map2(center_x, center_y, ~ circle(.x, .y, r))) %>%
    select(center, circles) %>%
    unnest()
  
  wedges = centers %>%
    mutate(data = pmap(list(cx = center_x, cy = center_y, r, n_wedge), wedges_f)) %>%
    select(center, data) %>%
    unnest() %>%
    mutate(group = interaction(center, wedge)) %>%
    mutate(fill = 1 * ring_scale(center, n_circle) + 1 * ring_scale(wedge, n_wedge))
  
  ggplot(NULL, aes(x, y)) +
    geom_path(data = circles, aes(group = center), color = 'black', size = 2) +
    geom_polygon(data = wedges, aes(group = group, fill = fill), color = 'black') +
    coord_fixed() +
    scale_fill_distiller(direction = 1, palette = 'Oranges', guide = 'none') +
    theme_void()
}

ring_viz(rep(1, 4), 0.5)
ring_viz(rep(6, 4), 0.5)
```

##### With different scales

Say we have $n_\uparrow$ superpopulations with $n_\downarrow$ populations each. The superpopulations mix with constant $\epsilon_\uparrow$ and the populations mix with constant $\epsilon_\downarrow$ within each superpopulation:
$$
\beta_{{i_\mu} \leftarrow {j_\nu}} = \begin{cases}
  \left[ 1 - \epsilon_\uparrow (1 - 1/n_\uparrow) \right] \times \left[ 1 - \epsilon_\downarrow (1 - 1/n_\downarrow) \right] & i = j, \mu = \nu \\
  \left[ 1 - \epsilon_\uparrow (1 - 1/n_\uparrow) \right] \times (\epsilon_\downarrow / n_\downarrow) & i = j, \mu \neq \nu \\
  (\epsilon_\uparrow / n_\uparrow) \times (1 / n_\downarrow) & i \neq j
\end{cases}
$$

### D-types model

#### Base model

**swo figure out how D-types works with N**

As a sensitivity analysis, I will repeat the analyses using the model presented by Lehtinen *et al*. (hereafter "D-types" model).

In the D-types model, there are $n_D$ "duration"-types, each with clearance rate $\mu_d$. Each type $d$ has sensitive and resistant strains, so the compartments are uncolonized $X$, sensitive-colonized $S_d$, and $R_d$. The D-types are subject to balancing selection, encoded by
$$
v_d = \left( 1 - \left[ \frac{S_d + R_d}{\sum_{d=1}^{n_D} (S_d + R_d)} - \frac{1}{n_D} \right]\right)^k,
$$
where $k \geq 1$ is the force of that selection. Thus:
$$
\begin{aligned}
\dot{S}_d &= v_d \beta S_d X - (\tau + \mu_d) S_d \\
\dot{R}_d &= v_d \frac{\beta}{c_\beta} R_d X - c_\mu \mu_d R_d \\
\end{aligned}
$$
where $\beta$ is the transmission coefficient, $\tau$ is the antibiotic treatment rate, $c_\beta \geq 1$ and $c_\mu \geq 1$ are the fitness costs of resistance.

The proportion resistant is
$$
\rho = \frac{\sum_d R_d}{\sum_d \left( S_d + R_d \right)}.
$$

#### Populations model

To extend this to multiple populations, we give each compartment an extra subscript $i$ and then need to be careful about the tranmission terms:
$$
\begin{aligned}
\dot{S}_{i,d} &= v_{i,d} \sum_j \beta_{i \leftarrow j} \frac{S_{j, d}}{N_j} X_i - \left( \tau_i + \mu_d \right) S_{i,d} \\
\dot{R}_{i,d} &= v_{i,d} \frac{1}{c_\beta} \sum_j \beta_{i \leftarrow j} \frac{R_{j, d}}{N_j} X_i - c_\mu \mu_d R_{i,d} \\
\end{aligned}
$$
where the $\beta_{i \leftarrow j}$ follow the same rules as for the WHN model.

#### Parameters

The parameter values are drawn from the paper: $n = 16$, $\beta = 2$, $c_\beta = 1$, $c_\mu = 1.1$, $k = 15$, $\tau \in [0.04, 0.20]$, $\mu_x$ evenly spaced in $[0.5, 2]$.

```{r dtypes_parameters}
# swo: some parms are lists, some are vectors. pick one?
dtypes_base_parms = list(
  n_d = 16,
  min_mu = 0.5,
  max_mu = 2.0,
  beta = 2.0,
  c_beta = 1.0,
  c_mu = 1.1,
  k = 15.0
)

dtypes_base_parms$mu_d = with(dtypes_base_parms,
                             seq(min_mu, max_mu, length = n_d))
```

# Results

## WHN transmission populations

```{r whn}
# vector to data frame
whn_unpack = function(x) {
  matrix(x, ncol = 4) %>%
    as_tibble() %>%
    setNames(c('X', 'S', 'R', 'D'))
}

# data frame to vector
whn_pack = function(df) {
  stopifnot(names(df) == c('X', 'S', 'R', 'D'))
  unlist(df)
}

whn_ode_func = function(t, state_vector, parms) {
  state = whn_unpack(state_vector)
  
  with(c(state, parms), {
    n_pop = nrow(state)
    stopifnot(dim(betaij) == rep(n_pop, 2))
    
    N = X + S + R + D
    
    dS = (betaij %*% ((S + D) / N)) * X - (u + taui) * S - (1 - cost) * (betaij %*% (R / N)) * S
    dR = (1 - cost) * (betaij %*% (R / N)) * X - u * R + taui * D
    dD = (1 - cost) * (betaij %*% (R / N)) * S - (u + taui) * D
    dX = -(dS + dR + dD)
    
    list(c(dX, dS, dR, dD))
  })
}

whn_sim = function(taui, epsilon) {
  n_pop = length(taui)

  parms = with(whn_base_parms, {
    betaij = matrix(beta * epsilon / n_pop, ncol = n_pop, nrow = n_pop)
    diag(betaij) <- beta * (1 - epsilon * (1 - 1 / n_pop))
    
    stopifnot(all(rowSums(betaij) == beta))
    stopifnot(all(colSums(betaij) == beta))
    
    c(whn_base_parms, list(
      taui = taui, epsilon = epsilon,
      betaij = betaij
    ))
  })
  
  state = data_frame(
    X = rep(0.990 / n_pop, n_pop),
    S = rep(0.005 / n_pop, n_pop),
    R = S,
    D = 0
  )
  
  state_vector = whn_pack(state)
  
  result = rootSolve::runsteady(state_vector, func = whn_ode_func, parms = parms)
  
  result$y %>%
    whn_unpack() %>%
    mutate(tau = taui,
           pop = seq_along(taui),
           rho = R / (S + R + D))
}

whn1 = crossing(
  n_pop = c(4, 8, 16),
  epsilon = c(0, 0.01, 0.1, 0.2, 1.0)
) %>%
  mutate(taui = map(n_pop, ~ seq(0.05, 0.2, length.out = .)),
         results = map2(taui, epsilon, whn_sim))
```

We are in one of the universes defined by $\epsilon$ and $n$. We don't know which one we're in.

```{r}
whn1 %>%
  unnest() %>%
  ggplot(aes(taui, rho, color = factor(epsilon))) +
  facet_grid(~ n_pop) +
  geom_point() +
  geom_line()

whn1 %>%
  mutate(linear_model = map(results, ~ lm(rho ~ tau, data = .)),
         slope = map_dbl(linear_model, ~ coef(.)['tau']),
         cil = map_dbl(linear_model, ~ confint(.)['tau', 1]),
         ciu = map_dbl(linear_model, ~ confint(.)['tau', 2])) %>%
  ggplot(aes(factor(epsilon), slope, ymin = cil, ymax = ciu)) +
  facet_wrap(~ n_pop) +
  geom_pointrange()
```

## WHN nested populations

```{r}
whn_nest_sim = function(taui, eps_super, eps_sub) {
  n_pop = length(taui)
  pop_per = sqrt(n_pop)
  
  if (pop_per != as.integer(pop_per)) {
    stop('length of taui must be a perfect square')
  }
  
  superpop = rep(1:pop_per, each = pop_per)
  stopifnot(length(superpop) == length(taui))
  
  contact = outer(1:n_pop, 1:n_pop, Vectorize(function(i, j) {
    if (i == j) {
      (1 - eps_super * (1 - 1 / pop_per)) * (1 - eps_sub * (1 - 1 / pop_per))
    } else if (superpop[i] == superpop[j]) {
      (1 - eps_super * (1 - 1 / pop_per)) * eps_sub / pop_per
    } else {
      eps_super / pop_per * 1 / pop_per
    }
  }))

  stopifnot(all.equal(rowSums(contact), rep(1.0, n_pop)))
  stopifnot(all.equal(colSums(contact), rep(1.0, n_pop)))

  parms = with(whn_base_parms, {
    betaij = beta * contact
    
    c(whn_base_parms, list(
      taui = taui, epsilon = epsilon,
      betaij = betaij
    ))
  })
  
  state = data_frame(
    X = rep(0.990 / n_pop, n_pop),
    S = rep(0.005 / n_pop, n_pop),
    R = S,
    D = 0
  )
  
  state_vector = whn_pack(state)
  
  result = rootSolve::runsteady(state_vector, func = whn_ode_func, parms = parms)
  
  result$y %>%
    whn_unpack() %>%
    mutate(pop = seq_along(taui),
           superpop = superpop[pop],
           tau = taui,
           rho = R / (S + R + D))
}

taui = rep(seq(0.07, 0.18, length.out = 4), each = 4) + 0.02 * rep(seq(-1, 1, length.out = 4), 4)

nest_results = crossing(
  eps_super = c(0.0, 0.01, 0.1, 0.5),
  eps_sub = c(0.0, 0.01, 0.1, 0.5)
) %>%
  filter(eps_super <= eps_sub) %>%
  mutate(data = map2(eps_super, eps_sub, ~ whn_nest_sim(taui, .x, .y)))

nest_results %>%
  unnest() %>%
  ggplot(aes(tau, rho, color = factor(superpop))) +
  facet_grid(eps_super ~ eps_sub, labeller = labeller(.rows = label_both, .cols = label_both)) +
  geom_hline(yintercept = 0.0) +
  geom_hline(yintercept = 0.5, color = 'gray', linetype = 2) +
  geom_smooth(aes(color = NULL), method = 'lm', se = FALSE, color = 'black', size = 0.5) +
  geom_point() +
  geom_line() +
  ylim(0 - 0.01, 1.01) +
  theme_classic()
```

The case where $\epsilon_\downarrow \gg 0$ means that there are effectively no subpopulations; each superpopulation is just one thing. In this sense, our analytical units are smaller than the relevant biology. The badness here is that we inflate the number of assertedly-independent observations, which means we have a greater chance of a Type I error. This seems like a small cost.

Going to larger scales, meaning aggregating, has the effect of shrinking the $x$ and $y$ axes, which has the cost of increasing the variance in the measurement. This is also a statistically bad thing.

### Going upwards

If we group *upwards*, treating many patches as if they were one, do we do any better? Not really: same slopes!

```{r}
kcluster = function(x, k) {
  if (length(x) < k) {
    stop('length(x) < k')
  } else if (k == length(x)) {
    1:k
  } else {
    kmeans(x, k)$cluster
  }
}

aggregate = function(df, groups) {
  df %>%
    mutate(group = groups) %>%
    group_by(group) %>%
    summarize_all(mean)
}

patches %>%
  mutate(
    model = map(results, ~ lm(rho ~ tau, data = .)),
    slope = map_dbl(model, ~ coef(.)['tau']),
    group = map(taui, ~ kcluster(., 4)),
    group_data = map2(results, group, aggregate),
    group_model = map(group_data, ~ lm(rho ~ tau, data = .)),
    group_slope = map_dbl(group_model, ~ coef(.)['tau'])
  ) %>%
  ggplot(aes(slope, group_slope)) +
  geom_abline() +
  geom_point(aes(color = factor(epsilon), shape = factor(n_patch)), size = 3)
```

## D-types

```{r dtype_helpers}
# pack the list into a vector
dtypes_pack = function(lst) {
  stopifnot(setequal(names(lst), c('X_i', 'S_id', 'R_id')))
  stopifnot(all(dim(lst$S_id) == dim(lst$R_id)))
  stopifnot(dim(lst$S_id)[1] == length(lst$X_i))
  c(lst$X_i, as.vector(lst$S_id), as.vector(lst$R_id))
}

# unpack the vector into a list
dtypes_unpack = function(x, n_pop, n_d) {
  stopifnot(length(x) == n_pop + 2 * (n_d * n_pop))

  x_end = n_pop
  s_start = x_end + 1
  s_end = x_end + n_d * n_pop
  r_start = s_end + 1
  r_end = s_end + n_d * n_pop
  stopifnot(r_end == length(x))

  list(X_i = x[1:x_end],
       S_id = matrix(x[s_start:s_end], nrow = n_pop, ncol = n_d),
       R_id = matrix(x[r_start:r_end], nrow = n_pop, ncol = n_d))
}

# for initially packing the data frame into the list
dtypes_df_to_list = function(df) {
  # check we have the right column names
  stopifnot(setequal(names(df), c('population', 'phenotype', 'dtype', 'value')))
  # check we have the right phenotypes
  stopifnot(setequal(df$phenotype, c('S', 'R', 'X')))

  # extract the no. populations and D-types
  n_pop = length(unique(df$population))
  n_d = length(unique(df$dtype))

  # there should be S and R for each pop/D-type combo
  stopifnot(nrow(filter(df, phenotype == 'S')) == n_pop * n_d)
  stopifnot(nrow(filter(df, phenotype == 'R')) == n_pop * n_d)
  # but X for only each pop
  stopifnot(nrow(filter(df, phenotype == 'X')) == n_pop)

  list(X_i = df %>% filter(phenotype == 'X') %$% values,
       S_id = df %>% filter(phenotype == 'S') %$% value %>% matrix(nrow = n_pop, ncol = n_d),
       R_id = df %>% filter(phenotype == 'R') %$% value %>% matrix(nrow = n_pop, ncol = n_d))
}

# for finally unpacking the list into a data frame
dtypes_list_to_df = function(lst) {
  # check for names
  stopifnot(setequal(names(lst), c('X_i', 'S_id', 'R_id')))

  # get dimensions
  n_pop = length(lst$X_i)
  n_d = dim(lst$S_id)[2]

  # check shapes
  stopifnot(all(dim(lst$S_id) == c(n_pop, n_d)))
  stopifnot(all(dim(lst$R_id) == c(n_pop, n_d)))

  X_rows = data_frame(pop = 1:n_pop, phenotype = 'X', dtype = NA, value = lst$X_i) 
  S_rows = crossing(dtype = 1:n_d, pop = 1:n_pop) %>%
    mutate(phenotype = 'S', value = as.vector(lst$S_id))
  R_rows = crossing(dtype = 1:n_d, pop = 1:n_pop) %>%
    mutate(phenotype = 'R', value = as.vector(lst$R_id))

  bind_rows(X_rows, S_rows, R_rows) %>%
    arrange(pop, phenotype, dtype)
}
```

```{dtypes_sim}
dtypes_ode_func = function(t, state_vector, parms) {
  state = dtypes_unpack(state_vector, n_pop = parms$n_pop, n_d = parms$n_d)

  with(c(state, parms), {
    stopifnot(length(X_i) == n_pop)
    stopifnot(dim(S_id) == c(n_pop, n_d))
    stopifnot(dim(R_id) == c(n_pop, n_d))

    # rowSums are over D-types (second index)
    v_id = (1.0 - ((S_id + R_id) / rowSums(S_id + R_id) - 1.0 / n_d)) ** k
    stopifnot(dim(v_id) == c(n_pop, n_d))

    N_i = X_i + rowSums(S_id + R_id)
    stopifnot(length(N_i) == n_pop)

    # "tau_i * S_id" does sum_i { tau_i S_id }, which is length P vector
    # to multiply by rows, need to do some fancy footwork: "mat %*% diag(row)"
    dS_id = v_id * (beta_ij %*% (S_id / N_i)) * X_i - tau_i * S_id - S_id %*% diag(mu_d)
    dR_id = v_id * (beta_ij %*% (R_id / N_i)) * X_i - c_mu * R_id %*% diag(mu_d)
    dX_i = -rowSums(dS_id + dR_id)

    list(dtypes_pack(list(X_i = dX_i, S_id = dS_id, R_id = dR_id)))
  })
}

dtypes_sim = function(tau_i, epsilon = 0) {
  n_pop = length(tau_i)

  parms = with(dtypes_base_parms, {
    beta_ij = matrix(beta * epsilon / n_pop, ncol = n_pop, nrow = n_pop)
    diag(beta_ij) <- beta * (1 - epsilon * (1 - 1 / n_pop))

    stopifnot(all(rowSums(beta_ij) == beta))
    stopifnot(all(colSums(beta_ij) == beta))

    c(dtypes_base_parms, list(
      n_pop = n_pop, tau_i = tau_i, epsilon = epsilon, beta_ij = beta_ij
    ))
  })
  
  n_d = parms$n_d
  
  state = list(X_i = rep(0.9 / n_pop, n_pop),
               S_id = matrix(0.05 / (n_pop * n_d), nrow = n_pop, ncol = n_d),
               R_id = matrix(0.05 / (n_pop * n_d), nrow = n_pop, ncol = n_d))

  state_vector = dtypes_pack(state)
  stopifnot(all.equal(sum(state_vector), 1))

  result = rootSolve::runsteady(state_vector, func = dtypes_ode_func, parms = parms,
                                stol = 1e-8 / n_pop, rtol = 1e-6 / n_pop, atol = 1e-6 / n_pop)

  result$y %>%
    dtypes_unpack(n_pop = n_pop, n_d = n_d) %>%
    dtypes_list_to_df() %>%
    left_join(data_frame(pop = 1:n_pop, tau = tau_i), by = 'pop')
}
```

```{r dtypes_results}
dtype1 = crossing(
  n_pop = c(4, 8, 16),
  epsilon = c(0, 0.01, 0.1, 0.2, 1.0)
) %>%
  mutate(tau_i = map(n_pop, ~ seq(0.05, 0.2, length.out = .)),
         results = map2(tau_i, epsilon, dtypes_sim))

dtype_summarize = function(df) {
  df %>%
    group_by(pop, phenotype) %>%
    summarize_at('value', sum) %>%
    ungroup() %>%
    spread(phenotype, value) %>%
    mutate(rho = R / (S + R))
}

dtype1 %>%
  mutate(results = map(results, dtype_summarize)) %>%
  unnest() %>%
  ggplot(aes(tau_i, rho, color = factor(epsilon))) +
  facet_wrap(~ n_pop) +
  geom_point() +
  geom_line()
```

## WHN nested

```{r dtypes_nest_sim}
dtypes_sim = function(tau_i, epsilon = 0) {
  n_pop = length(tau_i)

  parms = with(dtypes_base_parms, {
    beta_ij = matrix(beta * epsilon / n_pop, ncol = n_pop, nrow = n_pop)
    diag(beta_ij) <- beta * (1 - epsilon * (1 - 1 / n_pop))

    stopifnot(all(rowSums(beta_ij) == beta))
    stopifnot(all(colSums(beta_ij) == beta))

    c(dtypes_base_parms, list(
      n_pop = n_pop, tau_i = tau_i, epsilon = epsilon, beta_ij = beta_ij
    ))
  })
  
  n_d = parms$n_d
  
  state = list(X_i = rep(0.9 / n_pop, n_pop),
               S_id = matrix(0.05 / (n_pop * n_d), nrow = n_pop, ncol = n_d),
               R_id = matrix(0.05 / (n_pop * n_d), nrow = n_pop, ncol = n_d))

  state_vector = dtypes_pack(state)
  stopifnot(all.equal(sum(state_vector), 1))

  result = rootSolve::runsteady(state_vector, func = dtypes_ode_func, parms = parms,
                                stol = 1e-8 / n_pop, rtol = 1e-6 / n_pop, atol = 1e-6 / n_pop)

  result$y %>%
    dtypes_unpack(n_pop = n_pop, n_d = n_d) %>%
    dtypes_list_to_df() %>%
    left_join(data_frame(pop = 1:n_pop, tau = tau_i), by = 'pop')
}
```

```{r dtypes_nest_results}

```

## Empirical

# Discussion

As expected the slopes are shallower within islands, but they are a *lot* shallower.

This is especially stark in the D-types model. In that model, coexistence comes from the fact that there are a range of clearance rates $\mu_x$. Some of these rates support sensitives and some support resistants. Whether an individual personally use more or less antibiotics doesn't flip the entire population of D-types. It appears that the individual-level variations in use only weakly modulate the force of infection an individual feels from the rest of the population.

In the within-host model, the within-island slopes are not so immensely shallow, but they are markedly more shallow than the cross-island relationship. The results of the two within-island simulations (fixed-population and time-since-treatment) give nearly identical results.

These results suggest that the best way to change your risk of antibiotic resistance is to change your environment, not your antibiotic use. I always thought that the worst thing about going to the hospital isn't that you're likely to use more antibiotics; it's that you're exposed to way more resistant bugs.

The fact that the within-island slopes are really shallow implies that, if an analysis did go below the relevant length scale, you'd expect a quick drop-off in the slopes.

This toy moel is limited because there's only one length scale. In reality there aren't completely-separated, well-mixed islands; instead there's a network of length scales. I expect that having multiple length scales would soften this weaking of slopes with shorter length scales so that you don't see a sudden cliff.

# Appendix

## WHN time since treatment

### Model

Another way to model this within-island difference is to consider the time between individuals' treatments. This requires modifying the equations so that individuals can transfer between compartments at a single timestep (via colonization or recovery) or can move between time since treatment (via treatment or by "aging out"). I modify the within-host model to show this:
$$
\begin{aligned}
\dot{S}_t &= \beta (S_\bullet + P_\bullet) X_t - (u + \tau) S_t - k \beta (1-c) R_\bullet S_t - \mathbf{1}_{t \neq T} \alpha S_t + \mathbf{1}_{t \neq 1} \alpha S_{t-1} \\
\dot{R}_t &= \beta (1-c) R_\bullet X_t - (u + \tau) R_t + \mathbf{1}_{t=1} \tau (R_\bullet + D_\bullet) - \mathbf{1}_{t \neq T} \alpha R_t + \mathbf{1}_{t \neq 1} \alpha R_{t-1} \\
\dot{D}_t &= k \beta (1-c) R_\bullet S_t - (u + \tau) D_t - \mathbf{1}_{t \neq T} \alpha D_t + \mathbf{1}_{t \neq 1} \alpha D_{t-1} \\
\dot{X}_t &= -\beta (S_\bullet + D_\bullet) X_t - \beta (1-c) R_\bullet X_t + u (S_t + R_t + D_t) - \tau X_t + \\
  &\qquad \mathbf{1}_{t=1} \tau (S_\bullet + X_\bullet) - \mathbf{1}_{t \neq T} \alpha X_t + \mathbf{1}_{t \neq 1} \alpha X_{t-1}
\end{aligned}
$$
where the subscript $t$ indicates time since treatment ($t=1$ means 0 to 1 month since treatment, $t=2$ means 1-2 months, etc.), $T$ is the number of time brackets recorded, the bullet indicates summation ($S_\bullet = \sum_t S_t$), $\mathbf{1}$ is the indicator function, and $1/\alpha = 1 \text{ month}$ is the duration of the time bracket. Note that treatment moves individuals out of the $X_t$ and $R_t$ compartments not because they change colonization status but because they move individuals back to $X_1$ and $R_1$.

I set $T = 72$ (i.e., recording up to 6 years since treatment). These equations are also solved numerically, setting all initial populations into the "just treated" compartments $t=1$.

The resistance propotion $\rho_t = R_t / (S_t + R_t + D_t)$ depends on the time bracket. Individuals in bracket $t$, who were treated at most $t/\alpha$ ago, have "average use" $\alpha/t$.

### Results

```{r davies_time_sim, eval = FALSE}
drop_last = function(x) x[-length(x)]

davies_time_sim = function(tau, k = 1.0, tau50 = 1.5 / 12, alpha = 1.0, nT = 72) {
  # nT = number of time intervals
  parms = with(davies_base_parms,
             c(davies_base_parms, tau=tau,
               alpha = alpha, nT = nT,
               k = k, cost = cost_function(beta, u, k, tau50)))

  # state is a matrix:
  # rows are X, S, R, D (=S_R); columns are time brackets
  # set all compartments to 0 except for the "just-treated" bracket
  state = matrix(0, nrow = 4, ncol = parms$nT)
  state[,1] = c(0.998, 0.001, 0.001, 0.0)
  state_c = as.vector(state)
  stopifnot(sum(state_c) == 1.0)
  
ode_func = function(t, state_c, parms) {
    with(parms, {
      # unpack the state
      state = matrix(state_c, nrow = 4)
      stopifnot(ncol(state) == nT)
      X = state[1, ]
      S = state[2, ]
      R = state[3, ]
      D = state[4, ]
      
      Xo = sum(X)
      So = sum(S)
      Ro = sum(R)
      Do = sum(D)
      
      dX = -beta * (So + Do) * X - beta*(1-cost) * Ro * X + u * (S + R + D) - tau * X + c(tau * (So + Xo), rep(0, nT - 1)) - c(alpha * drop_last(X), 0) + c(0, alpha * drop_last(X))
      dS = beta * (So + Do) * X - (u + tau) * S - k * beta * (1-cost) * Ro * S - c(alpha * drop_last(S), 0) + c(0, alpha * drop_last(S))
      dR = beta*(1-cost) * Ro * X - (u + tau) * R + c(tau * (Ro + Do), rep(0, nT - 1)) - c(alpha * drop_last(R), 0) + c(0, alpha * drop_last(R))
      dD = k * beta*(1-cost) * Ro * S - (u + tau) * D - c(alpha * drop_last(D), 0) + c(0, alpha * drop_last(D))
      
      dstate = rbind(dX, dS, dR, dD)
      dstate_c = as.vector(dstate)
      stopifnot(abs(sum(dstate_c)) < 1e-6)
      list(dstate_c)
    })
  }

  result = rootSolve::runsteady(state_c, func = ode_func, parms = parms)
  end_state = result$y %>% matrix(nrow=4)
  
  time = 1:parms$nT
  bind_rows(
    data_frame(compartment = 'X', time, y = end_state[1,]),
    data_frame(compartment = 'S', time, y = end_state[2,]),
    data_frame(compartment = 'R', time, y = end_state[3,]),
    data_frame(compartment = 'D', time, y = end_state[4,])
  )
}

davies_time = crossing(tau = round(c(1.25, 1.5, 1.75) / 12, 3)) %>%
  mutate(model = map(tau, davies_time_sim)) %>%
  unnest() %>%
  rename(big_tau = tau) %>%
  spread(compartment, y) %>%
  mutate(tau = 1 / time,
         rho = R / (S + R + D))
```

```{r davies_time_plot, eval = FALSE}
davies_time %>%
  ggplot(aes(tau, rho)) +
  geom_line(data = davies_cross) +
  geom_point(aes(color = factor(big_tau)), shape = 2) +
  geom_line(aes(color = factor(big_tau)), linetype = 2)
```

For the time simulation, I verify that there are enough time compartments so that they long-time-since-treatment compartments are nearly empty. The columns are the $\tau$.

```{r confirm_time_comparments, eval = FALSE}
davies_time %>%
  gather('compartment', 'value', D:X) %>%
  ggplot(aes(time, value, color = compartment)) +
  geom_line() +
  facet_grid(~ big_tau) +
  xlab('Time since treatment (months)') +
  ylab('Proportion')
```

I compare the cross-island, within-island fixed-population, and within-island recorded-time simulations by plotting all the results on the same plot. The black lines show the cross-island results (i.e., the results from Davies *et al*. Figure 2e, above). The circles show results from the within-island fixed-population simulation. The colored lines show the within-island recorded-time simulation. The color of the circles and lines shows the island-level average $\tau$ used in the simulations. (You can read off those values by looking at where the colored lines intercept the black lines: when the island-level $\tau$ and within-island use are the same, you get the same results.) Panels show values of co-colonization efficiency $k$.

```{r davies_time_comp, eval = FALSE}
davies_time_comp = davies_time %>% filter(tau < 0.3)

ggplot(data = NULL, aes(tau, rho)) +
  geom_line(data = filter(davies_cross, k == 1.0)) +
  geom_point(data = davies_small, aes(color = factor(big_tau)), shape = 1) +
  geom_line(data = davies_small, aes(color = factor(big_tau))) +
  geom_point(data = davies_time_comp, aes(color = factor(big_tau)), shape = 2) +
  geom_line(data = davies_time_comp, aes(color = factor(big_tau)), linetype = 2)
```

---

# Old

## D-types original figure

I reproduce Figure 3 from Lehtinen *et al*.:

```{r}
# reproduce the paper figure
dtypes_sim(tau_i = c(0.075)) %>%
  filter(phenotype != 'X') %>%
  ggplot(aes(factor(dtype), value, fill = phenotype)) +
  geom_col()
```
