---
title: "Empirical use-resistance relationships over geographic scales"
author: "Scott Olesen"
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(
  fig.width = 7, fig.height = 4, fig.path = 'fig/',
  dev = c('png', 'pdf'), 
  echo = FALSE, warning = FALSE, message = FALSE,
  cache = TRUE, autodep = TRUE)

pdf.options(useDingbats = FALSE, useKerning = FALSE)

map = purrr::map
```

# Methods

## Data

### MarketScan

```{r maup_db}
state_data = read_tsv('../db/state_census.tsv')
```

```{r marketscan_data}
marketscan_res = read_tsv('~/grad/proj/medicare/analysis/ms2/data/abg_state.tsv') %>%
  rename(drug = drug_group) %>%
  mutate(bugdrug = case_when(
    .$bug == 'E. coli' & .$drug == 'quinolone' ~ 'Ec/q',
    .$bug == 'S. pneumoniae' & .$drug == 'beta_lactam' ~ 'Sp/bl',
    .$bug == 'S. pneumoniae' & .$drug == 'macrolide' ~ 'Sp/m'
  )) %>%
  filter(!is.na(bugdrug)) %>%
  mutate(n_resistant = as.integer(round(f_ns * n_isolates))) %>%
  select(bugdrug, drug, state, n_resistant, n_isolates)

marketscan_use = read_tsv('~/grad/proj/medicare/analysis/ms2/ineq_marketscan.tsv') %>%
  rename(drug = drug_group, use = total_use) %>%
  filter(drug %in% c('quinolone', 'beta_lactam', 'macrolide'))

marketscan = marketscan_use %>%
  inner_join(marketscan_res, by = c('drug', 'state')) %>%
  rename(unit = state) %>%
  left_join(select(state_data, unit, population), by = 'unit') %>%
  select(unit, bugdrug, use, n_resistant, n_isolates, population)
```

### NHSN/IMS

```{r nhsn_ims}
# nhsn1 = read_tsv('../data/nhsn-ims/data.tsv') %>%
#   mutate(dataset = 'NHSN/IMS', bugdrug = 'Ec/q',
#          use = rx_1k_year / 1e3) %>%
#   select(dataset, bugdrug, unit = state, use, n_resistant, n_isolates)

nhsn = read_tsv('../data/nhsn-ims/data.tsv') %>%
  rename(state_abbreviation = state) %>%
  filter(state_abbreviation %in% state.abb) %>%
  mutate(state_id = match(state_abbreviation, state.abb),
         unit = state.name[state_id],
         bugdrug = 'Ec/q',
         use = rx_1k_year / 1e3) %>%
  arrange(unit) %>%
  left_join(select(state_data, unit, population), by = 'unit') %>%
  select(unit, bugdrug, use, n_resistant, n_isolates, population)
```

### ECDC

For the European data, we convert to "treatments" per person per year, assuming a certain conversion from DDD to treatments. This is just an *ad hoc* adjustment to get the European data to the same scales as used in the simulations and the US data.

```{r ecdc_convert}
did_cpy_map = data_frame(
  drug = c('beta_lactam', 'quinolone', 'macrolide'),
  ddd_per_tx = c(10, 10, 7),
  cpy_per_did = 365 / (1e3 * ddd_per_tx)
)

did_cpy_map %>%
  kable(caption = 'DDD to treatment conversion')
```

```{r ecdc}
europe_units = read_tsv('../db/europe_units.tsv') %>%
  select(unit = country, long = longitude, lat = latitude, population, unit_id, region_name)

europe = read_tsv('../data/ecdc/data.tsv') %>%
  left_join(did_cpy_map, by = 'drug') %>%
  mutate(use = cpy_per_did * did) %>%
  mutate(bugdrug = case_when(
    .$bug == 'Escherichia coli' & .$drug == 'quinolone' ~ 'Ec/q',
    .$bug == 'Streptococcus pneumoniae' & .$drug == 'beta_lactam' ~ 'Sp/bl',
    .$bug == 'Streptococcus pneumoniae' & .$drug == 'macrolide' ~ 'Sp/m'
  )) %>%
  rename(unit = country) %>%
  left_join(select(europe_units, unit, population), by = 'unit') %>%
  select(unit, bugdrug, use, n_resistant = n_ns, n_isolates, population)

europe_names = unique(europe$unit)
```

# Results

## MarketScan

```{r marketscan_maup_setup}
# group n items into k groups, with at least n_min items in each group
sample_nmin = function(n, k, n_min = 2) {
  stopifnot(n_min * k <= n)
  sample(c(rep(1:k, each = n_min), sample(1:k, n - (n_min * k), replace = TRUE)))
}

renumber = function(x) match(x, unique(x))

# given distances between things, create groups
random_zones = function(distances, k) {
  stopifnot(nrow(distances) == ncol(distances))
  stopifnot(nrow(distances) >= k)
  
  centers = sample(1:nrow(distances), k)
  zones = distances[, centers] %>%
    apply(1, function(x) centers[which.min(x)])
  
  renumber(zones)
}

# wrapper around random zones, to ensure group size
random_zones_nmin = function(distances, k, n_min = 2, max_tries = 1e2) {
  try = 1
  while (try < max_tries) {
    z = random_zones(distances, k)
    if (min(table(z)) >= n_min) return(z)
    try = try + 1
  }
  
  NULL
}

deg2rad = function(deg) deg * pi / 180

haversine = function(long1, lat1, long2, lat2, R = 6371) {
  long1 = deg2rad(long1)
  lat1 = deg2rad(lat1)
  long2 = deg2rad(long2)
  lat2 = deg2rad(lat2)
  R * acos(pmin(1.0, sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(long2 - long1)))
}

unit_distances = function(unit_data) {
  stopifnot(all(c('unit', 'long', 'lat') %in% names(unit_data)))
  if (any(duplicated(unit_data$unit))) stop('duplicated units')

  units = unit_data$unit
  n_units = length(units)
  
  unit_data %>%
    crossing(., .) %>%
    arrange(unit, unit1) %>%
    mutate(dist = haversine(long, lat, long1, lat1)) %$%
    matrix(dist, ncol = n_units, nrow = n_units) %>%
    set_rownames(units) %>%
    set_colnames(units)
}
```

```{r maup_helper_functions}
aggregate = function(df, group) {
  mutate(df, group = group) %>%
    group_by(group) %>%
    summarize(use = weighted.mean(use, w = population),
              n_resistant = sum(n_resistant),
              n_isolates = sum(n_isolates)) %>%
    mutate(n_susceptible = n_isolates - n_resistant,
           res = n_resistant / n_isolates)
}

group_models = function(df, group) {
  df %>%
    mutate(res = n_resistant / n_isolates) %>%
    split(group) %>%
    map(~ lm(res ~ use, data = .))
}

try_load = function(fn, expr, force_run = FALSE) {
  if (!force_run && file.exists(fn)) {
    read_rds(fn)
  } else {
    value = expr
    write_rds(value, fn)
  }
}
```

```{r marketscan_maup_compute}
# create a hierarchical list: marketscan_maup_data$`Ec/q`$data
marketscan_maup_data = marketscan %>%
  nest(-bugdrug) %>%
  mutate(
    state_id = map(data, ~ match(.$unit, state.name)),
    unit_data = map(data, ~ filter(state_data, unit %in% .$unit)),
    n_units = map_int(unit_data, nrow),
    unit_distances = map(unit_data, unit_distances)
  ) %>%
  gather('key', 'value', -bugdrug) %>%
  nest(key, value, .key = 'value_tbl') %>%
  mutate(value_list = map(value_tbl, ~ setNames(.$value, .$key))) %$%
  setNames(value_list, bugdrug)

maup_group = function(maup_data, k, group_type) {
  switch(group_type,
         'sample' = sample_nmin(maup_data$n_units, k),
         'zone' = random_zones_nmin(maup_data$unit_distances, k),
         'region' = renumber(state.region[maup_data$state_id]),
         'division' = renumber(state.division[maup_data$state_id]),
         'state' = maup_data$state_id,
         'europe_region' = renumber(europe_units$region_name[maup_data[[.y]]$stat_id])
  )
}

n_iterations = 100

marketscan_canonical = crossing(
  bugdrug = marketscan$bugdrug,
  group_type = c('region', 'division', 'state')
)

add_canonical_data = function(df, maup_data) {
  df %>%
    mutate(group = map2(group_type, bugdrug,
                        ~ switch(.x,
                                 'region' = renumber(state.region[maup_data[[.y]]$state_id]),
                                 'division' = renumber(state.division[maup_data[[.y]]$state_id]),
                                 'state' = maup_data[[.y]]$state_id,
                                 'europe_region' = renumber(europe_units$region_name[maup_data[[.y]]$state_id])
                                 )),
           k = map_int(group, ~ length(unique(.))))
}

maup_sim = function(bugdrug, k, n_iterations, canonical_rows, maup_data, fn, force_run = FALSE) {
  try_load(fn, {
    crossing(
      bugdrug = bugdrug,
      k = k,
      group_type = c('sample', 'zone'),
      iteration = 1:n_iterations
    ) %>%
      mutate(group = pmap(list(bugdrug, k, group_type), ~ maup_group(maup_data[[..1]], ..2, ..3))) %>%
      bind_rows(add_canonical_data(canonical_rows, maup_data)) %>%
      mutate(
        group_data = map2(bugdrug, group, ~ aggregate(maup_data[[.x]]$data, .y)),
        group_type = if_else(group_type %in% c('state', 'region', 'division', 'europe_region'), 'canonical', group_type),
        super_model = map(group_data, ~ lm(res ~ use, data = .)),
        super_slope = map_dbl(super_model, ~ coef(.)['use']),
        sub_models = map2(bugdrug, group, ~ group_models(maup_data[[.x]]$data, .y)),
        sub_slopes = map(sub_models, function(ms) map_dbl(ms, ~ coef(.)['use'])),
        median_sub_slope = map_dbl(sub_slopes, ~ median(., na.rm = TRUE)),
        slope_ratio = super_slope / median_sub_slope,
        # weighted linear models
        w_super_model = map(group_data, ~ lm(res ~ use, weights = n_isolates, data = .)),
        w_super_slope = map_dbl(w_super_model, ~ coef(.)['use'])
      )
  }, force_run = force_run)
}

marketscan_maup = maup_sim(
  bugdrug = marketscan$bugdrug,
  k = c(4, 9),
  n_iterations = 100,
  canonical_rows = marketscan_canonical,
  maup_data = marketscan_maup_data,
  fn = 'results/marketscan_maup.rds',
  force_run = TRUE
)
```

### Canonical groupings

```{r marketscan_canonical}
marketscan_canonical_data = marketscan_maup %>%
  filter(group_type == 'canonical') %>%
  mutate(level = case_when(
    .$k == 4 ~ 'region',
    .$k == 9 ~ 'division',
    TRUE ~ 'state'
  )) %>%
  select(bugdrug, level, data = group_data)
  
marketscan_canonical_data %>%
  unnest() %>%
  ggplot(aes(use, res, color = level)) +
  facet_wrap(~ bugdrug, scales = 'free') +
  geom_smooth(aes(fill = level), method = 'lm', alpha = 0.1) +
  geom_point()

marketscan_canonical_data %>%
  mutate(
    model = map(data, ~ lm(res ~ use, data = .)),
    slope = map_dbl(model, ~ coef(.)['use']),
    slope_cil = map_dbl(model, ~ confint(.)['use', 1]),
    slope_ciu = map_dbl(model, ~ confint(.)['use', 2])
  ) %>%
  select(bugdrug, level, starts_with('slope')) %>%
  kable(caption = 'Slopes')
```

### MAUP

```{r marketscan_maup_table}
maup_table = function(maup, base_level = 'state', ks = c(4, 9)) {
  base_slopes = maup %>%
    filter(group_type == 'canonical') %>%
    group_by(bugdrug) %>%
    filter(k == max(k)) %>%
    ungroup() %>%
    select(bugdrug, base_slope = super_slope)
  
  maup %>%
    filter(group_type != 'canonical' | k %in% ks) %>%
    select(bugdrug, k, group_type, super_slope) %>%
    nest(super_slope) %>%
    mutate(
      data = map(data, ~ unlist(., use.names = FALSE)),
      median_super = map_dbl(data, median),
      iqr_super = map_dbl(data, IQR)
    ) %>%
    left_join(base_slopes, by = 'bugdrug') %>%
    mutate(
      wilcox_p = map2_dbl(data, base_slope, ~ wilcox.test(.x - .y, alternative = 'greater')$p.value),
      fdr = p.adjust(wilcox_p, 'BH') < 0.05
    ) %>%
    mutate(wilcox_p = if_else(group_type == 'canonical', NA_real_, wilcox_p),
           log10p = log10(wilcox_p),
           fdr = if_else(group_type == 'canonical', NA, fdr)) %>%
    select(bugdrug, k, group_type, base_slope, median_super, iqr_super, wilcox_p, log10p, fdr) %>%
    mutate_if(is.numeric, ~ round(., 3)) %>%
    arrange(bugdrug, group_type, k) %>%
    kable(caption = 'Comparing base and MAUP slopes')  
}

maup_table(marketscan_maup)
```

Histogram of slopes over the random aggregates (zonal and sample) for each bug/drug and $k$. Dotted blue line shows the slope over the raw units (states).

```{r marketscan_maup_plot}
maup_plot = function(maup) {
  dat = maup %>%
    filter(group_type != 'canonical') %>%
    select(bugdrug, k, group_type, super_slope)
  
  slopes = maup %>%
    filter(group_type == 'canonical') %>%
    group_by(bugdrug) %>%
    filter(k == max(k)) %>%
    ungroup() %>%
    select(bugdrug, base_slope = super_slope) %>%
    crossing(k = unique(dat$k))
  
  dat %>%
    ggplot(aes(super_slope, fill = group_type)) +
    facet_grid(k ~ bugdrug, scales = 'free') +
    geom_vline(
      data = slopes,
      aes(xintercept = base_slope),
      color = 'blue', linetype = 2
    ) +
    geom_histogram(position = 'dodge') +
    xlab('slope over random aggregate')
}

maup_plot(marketscan_maup)
```

## IMS/NHSN CAUTIs

```{r nhsn}
nhsn_maup_data = nhsn %>%
  nest(-bugdrug) %>%
  mutate(
    state_id = map(data, ~ match(.$unit, state.name)),
    unit_data = map(data, ~ filter(state_data, unit %in% .$unit)),
    n_units = map_int(unit_data, nrow),
    unit_distances = map(unit_data, unit_distances)
  ) %>%
  gather('key', 'value', -bugdrug) %>%
  nest(key, value, .key = 'value_tbl') %>%
  mutate(value_list = map(value_tbl, ~ setNames(.$value, .$key))) %$%
  setNames(value_list, bugdrug)

nhsn_canonical = crossing(
  bugdrug = 'Ec/q',
  group_type = c('region', 'division', 'state')
)

nhsn_maup = maup_sim(
  bugdrug = 'Ec/q',
  k = c(4, 9),
  n_iterations = 100,
  canonical_rows = nhsn_canonical,
  maup_data = nhsn_maup_data,
  fn = 'results/nhsn_maup.rds',
  force_run = TRUE
)
```

### Canonical groupings

```{r nhsn_canonical}
nhsn_canonical_data = nhsn_maup %>%
  filter(group_type == 'canonical') %>%
  mutate(level = case_when(
    .$k == 4 ~ 'region',
    .$k == 9 ~ 'division',
    TRUE ~ 'state'
  )) %>%
  select(bugdrug, level, data = group_data)

nhsn_canonical_data %>%
  unnest() %>%
  ggplot(aes(use, res, color = level)) +
  facet_wrap(~ bugdrug, scales = 'free') +
  geom_smooth(aes(fill = level), method = 'lm', alpha = 0.1) +
  geom_point()

nhsn_canonical_data %>%
  mutate(model = map(data, ~ lm(res ~ use, data = .)),
         slope = map_dbl(model, ~ coef(.)['use']),
         lci = map_dbl(model, ~ confint(.)['use', 1]),
         uci = map_dbl(model, ~ confint(.)['use', 2])) %>%
  select(bugdrug, level, slope, lci, uci)
```

### MAUP

```{r nhsn_maup_table}
maup_table(nhsn_maup)
```

**swo** refactor this in the same style as marketscan. also combine it with Europe at the same time?

```{r nhsn_maup_plot}
maup_plot(nhsn_maup)
```

### Example regions

```{r hex}
hex_map = read_tsv('~/grad/db/us_hex.tsv') %>%
  filter(!is.na(state)) %>%
  rename(unit = state)

hex_labels = hex_map %>%
  filter(order %in% c(0, 3)) %>%
  group_by(unit) %>%
  summarize_at(c('x', 'y'), mean) %>%
  mutate(abb = state.abb[match(unit, state.name)]) %>%
  crossing(k = c(4, 9), group_type = c('canonical', 'sample', 'zone'))

hex_borders = hex_map %>%
  crossing(k = c(4, 9), group_type = c('canonical', 'sample', 'zone'))

nhsn_maup %>%
  filter(group_type == 'canonical' | iteration == 1, k %in% c(4, 9)) %>%
  mutate(unit = map(group, ~ nhsn_maup_data$`Ec/q`$unit_data$unit)) %>%
  select(k, group_type, group, unit) %>%
  unnest() %>%
  right_join(hex_borders, by = c('unit', 'k', 'group_type')) %>%
  ggplot(aes(x, y, group = unit)) +
  facet_grid(group_type ~ k) +
  geom_polygon(aes(fill = factor(group)), color = 'black', show.legend = FALSE) +
  geom_text(data = hex_labels, aes(label = abb)) +
  coord_fixed() +
  theme_void()
```

## Europe

```{r europe_data}
europe_maup_data = europe %>%
  nest(-bugdrug) %>%
  mutate(
    state_id = map(data, ~ match(.$unit, europe_names)),
    unit_data = map(data, ~ filter(europe_units, unit %in% .$unit)),
    n_units = map_int(unit_data, nrow),
    unit_distances = map(unit_data, unit_distances)
  ) %>%
  gather('key', 'value', -bugdrug) %>%
  nest(key, value, .key = 'value_tbl') %>%
  mutate(value_list = map(value_tbl, ~ setNames(.$value, .$key))) %$%
  setNames(value_list, bugdrug)

# swo: come up with a better name than "state"
europe_canonical = crossing(
  bugdrug = europe$bugdrug,
  group_type = c('state', 'europe_region')
)

europe_maup = maup_sim(
  bugdrug = unique(europe$bugdrug),
  k = c(4),
  n_iterations = 100,
  canonical_rows = europe_canonical,
  maup_data = europe_maup_data,
  fn = 'results/europe_maup.rds',
  force_run = TRUE
)
```

### Canonical

```{r europe_canonical}
europe_maup %>%
  filter(group_type == 'canonical') %>%
  mutate(level = case_when(
    .$k == 4 ~ 'region',
    .$k == 9 ~ 'division',
    TRUE ~ 'state'
  )) %>%
  select(bugdrug, level, group_data) %>%
  unnest() %>%
  ggplot(aes(use, res, color = level)) +
  facet_wrap(~ bugdrug, scales = 'free') +
  geom_smooth(aes(fill = level), method = 'lm', alpha = 0.1) +
  geom_point()
```

### MAUP

```{r europe_maup_table}
maup_table(europe_maup)
```

```{r europe_maup_plot}
maup_plot(europe_maup)
```

## Canonical grouping analysis

### Significance test

To see if the observed (canonical) aggregate slope is anything special, compare it to the regional slopes you would get from permuting the region labels.

```{r aggregate_slope_analysis}
agg_slope_f = function(df, group, permute = FALSE) {
  expected_names = c(group, 'use', 'population', 'n_resistant', 'n_isolates')
  stopifnot(all(expected_names %in% names(df)))
  
  group_ = as.symbol(group)
  
  permute_f = if (permute) {
    function(x) mutate(x, !!group := sample(!!group_))
  } else {
    identity
  }
  
  df %>%
    permute_f() %>%
    group_by(!!group_) %>%
    summarize(use = weighted.mean(use, w = population),
              n_resistant = sum(n_resistant),
              n_isolates = sum(n_isolates)) %>%
    mutate(n_susceptible = n_isolates - n_resistant,
           res = n_resistant / n_isolates) %>%
    lm(res ~ use, data = .) %>%
    coef() %>%
    extract('use') %>%
    unname()
}

# group as character
agg_slope_p = function(df, group, n_iter) {
  obs_slope = agg_slope_f(df, group, permute = FALSE)
  perm_slopes = replicate(n_iter, agg_slope_f(df, group, permute = TRUE))
  
  r = sum(perm_slopes > obs_slope)
  (r + 1) / (n_iter + 1)
}

agg_slope = function(df, unit_data, groups, n_iter = 99) {
  dfn = quo_name(enquo(df))
  mutate(df) %>%
    left_join(unit_data, by = c('unit', 'population')) %>%
    nest(-bugdrug) %>%
    crossing(group = groups) %>%
    mutate(p_value = map2_dbl(data, group, ~ agg_slope_p(.x, .y, n_iter)),
           dataset = dfn)
}

agg_slope_results = bind_rows(
  agg_slope(marketscan, state_data, c('region', 'division')),
  agg_slope(nhsn, state_data, c('region', 'division')),
  agg_slope(europe, europe_units, c('region_name'))
)

agg_slope_results %>%
  mutate(fdr = p.adjust(p_value, 'BH') < 0.05) %>%
  select(dataset, bugdrug, group, p_value, fdr) %>%
  kable(caption = 'Empiricial tests, swapping the group labels')
```

## Distance analysis

If local tranmission "washes out" the use/resistance relationship, then we expect that pairs of states that are closer to one another will have a greater difference in resistance for the same difference in use. That is:
$$
\frac{\Delta \rho}{\Delta \tau} \sim d,
$$
where $d$ is the distance between the two units.

In these analyses, we'll zero-center $d$ by subtracting its mean so that the regression intercept gives the expected ARRU at "average" distances. This should be comparable to the slope in $\rho \sim \tau$ across units.

```{r dist_point}
odds = function(p) p / (1 - p)
odds_ratio = function(p, q) odds(p) / odds(q)
coef1 = function(model, term) model %>% coef() %>% extract(term) %>% unname()

# turn the bugdrug bundle into a pairwise list of dr, du, and distances
distance_fit_data = function(bd, permute = FALSE) {
  # prepare the data set
  d = bd$data %>%
    mutate(res = n_resistant / n_isolates,
           i = 1:n()) %>%
    select(unit, i, use, res)
  
  if (permute) {
    d %<>% mutate(i = sample(i))
  }
  
  # get all combinations of units
  units = unique(d$unit)
  crossing(unit1 = units, unit2 = units) %>%
    filter(unit1 < unit2) %>%
    left_join(d, by = c('unit1' = 'unit')) %>%
    left_join(d, by = c('unit2' = 'unit'), suffix = c('1', '2')) %>%
    mutate(dist = map2_dbl(i1, i2, ~ bd$unit_distances[.x, .y]),
           dr = res2 - res1,
           du = use2 - use1,
           arru = dr / du,
           loru = log(odds_ratio(res2, res1)) / du,
           d = dist - mean(dist))
}

bisquare = partial(MASS::rlm, psi = MASS::psi.bisquare)

regression_p = function(bd, n_iter = 99, method = bisquare) {
  vals = function(permute) {
    dfd = distance_fit_data(bd, permute = permute)
    
    arru = method(arru ~ d, data = dfd) %>%
      coef1('d')
    
    loru = method(loru ~ d, data = dfd) %>%
      coef1('d')
    
    c('arru' = arru, 'loru' = loru)
  }
  
  base_vals = vals(FALSE)
  perm_vals = replicate(n_iter, vals(TRUE))
  
  r = rowSums(perm_vals > base_vals)
  
  (r + 1) / (n_iter + 1)
}

# uses the t values reported by MASS::rlm
bs_model_p = function(model) {
  model %>%
    summary() %>%
    coef() %>%
    { .['d', 't value'] } %>%
    { -abs(.) } %>%
    pt(df = length(model$residuals) - 2)
}

confint.rlm = function(m) {
  cs = coef(summary(m))
  estimate = cs[, 'Value']
  se = cs[, 'Std. Error']
  result = cbind(estimate - 1.96 * se, estimate + 1.96 * se)
  rownames(result) <- names(estimate)
  colnames(result) <- c('2.5%', '97.5%')
  result
}

distance_fit = function(dataset, lst) {
  data_frame(
    dataset = dataset,
    bugdrug = names(lst),
    data = map(lst, distance_fit_data),
    model_arru = map(data, ~ bisquare(arru ~ d, data = .)),
    model_loru = map(data, ~ bisquare(loru ~ d, data = .)),
    p_values = map(lst, regression_p),
    arru_p = map_dbl(p_values, ~ .['arru']),
    loru_p = map_dbl(p_values, ~ .['loru'])
  )
}

distance_fit_results = bind_rows(
  distance_fit('marketscan', marketscan_maup_data),
  distance_fit('nhsn', nhsn_maup_data),
  distance_fit('europe', europe_maup_data)
)

distance_fit_table = function(model, p) {
  model_ = enquo(model)
  p_ = enquo(p)

  distance_fit_results %>%
    select(dataset, bugdrug, model = !!model_) %>%
    mutate(vals = map(model, tidy)) %>%
    select(-model) %>%
    unnest() %>%
    mutate_at(c('estimate', 'std.error'), function(x) if_else(.$term == 'd', 1e3 * x, x)) %>%
    mutate(hci = 1.96 * std.error,
           cil = estimate - hci,
           ciu = estimate + hci,
           range = sprintf('%.3f (%.3f to %.3f)', estimate, cil, ciu)) %>%
    select(dataset, bugdrug, term, range) %>%
    spread(term, range) %>%
    rename(slope = d) %>%
    left_join(select(distance_fit_results, dataset, bugdrug, slope_p = !!p_)) %>%
    mutate(fdr = p.adjust(slope_p, 'BH') < 0.05) %>%
    arrange(dataset, bugdrug)
}

distance_fit_table(model_arru, arru_p) %>%
  kable(caption = 'Point estimates of intercept (ARRU, in p.p. resistance per unit use) and slope (ARRU per 10^3 km).')

distance_fit_table(model_loru, loru_p) %>%
  kable(caption = 'Point estimates of intercept (LORU, in per unit use) and slope (LORU per 10^3 km).')
```

In the robust regressions, all the point estimates are positive, and some $p$-values are below $0.05$, but none pass an FDR $Q < 0.05$. Interestingly, the point estimates are fairly similar, after accounting for the fact that the European metric is ~10x the US metric.

These results suggest that an increased sitance of $10^3$ km leads to an ~10% improvement in $\Delta \rho / \Delta \tau$.
